{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N.A.T.E - Fake News Detector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WiVSuTuOeqV9"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fir121/NATE-Fake-News-AI/blob/main/N_A_T_E_Fake_News_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiVSuTuOeqV9"
      },
      "source": [
        "# Background code for N.A.T.E"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewFL_vL20vMS",
        "outputId": "3b7719da-eae3-45c8-e5e5-79678b96f0a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "import pickle\n",
        "\n",
        "import requests, io, zipfile\n",
        "# Download class resources...\n",
        "r = requests.get(\"https://www.dropbox.com/s/2pj07qip0ei09xt/inspirit_fake_news_resources.zip?dl=1\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "basepath = '.'\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "with open(os.path.join(basepath, 'train_val_data.pkl'), 'rb') as f:\n",
        "  train_data, val_data = pickle.load(f)\n",
        "  \n",
        "print('Number of train examples:', len(train_data))\n",
        "print('Number of val examples:', len(val_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train examples: 2002\n",
            "Number of val examples: 309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49IV8jyXSbt5"
      },
      "source": [
        "def get_description_from_html(html):\n",
        "  soup = bs(html)\n",
        "  description_tag = soup.find('meta', attrs={'name':'og:description'}) or soup.find('meta', attrs={'property':'description'}) or soup.find('meta', attrs={'name':'description'})\n",
        "  if description_tag:\n",
        "    description = description_tag.get('content') or ''\n",
        "  else: # If there is no description, return empty string.\n",
        "    description = ''\n",
        "  return description\n",
        "\n",
        "def scrape_description(url):\n",
        "  if not url.startswith('http'):\n",
        "    url = 'http://' + url\n",
        "  response = requests.get(url, timeout=10)\n",
        "  html = response.text\n",
        "  description = get_description_from_html(html)\n",
        "  return description"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJQY3DU1Iza9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cf95ce4-e107-42fa-fb50-fd583cc66af2"
      },
      "source": [
        "def get_descriptions_from_data(data):\n",
        "  # A dictionary mapping from url to description for the websites in \n",
        "  # train_data.\n",
        "  descriptions = []\n",
        "  d=''\n",
        "  for site in tqdm(data):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    descriptions.append(get_description_from_html(site[1]))\n",
        "\n",
        "    ### END CODE ###\n",
        "  return descriptions\n",
        "  \n",
        "\n",
        "train_descriptions = get_descriptions_from_data(train_data)\n",
        "train_urls = [url for (url, html, label) in train_data]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2002/2002 [02:23<00:00, 13.99it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPuUgA39bO3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31f6f821-9a78-4165-e5a2-8ebdc6aec38c"
      },
      "source": [
        "val_descriptions = get_descriptions_from_data(val_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 309/309 [00:21<00:00, 14.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qduz6WP1JBCO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b3efad04-c4e5-427c-91c8-14072b446e21"
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=300)\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(descriptions, vectorizer):\n",
        "  X = vectorizer.transform(descriptions).todense()\n",
        "  return X\n",
        "\n",
        "print('\\nPreparing train data...')\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_train_y = [label for url, html, label in train_data]\n",
        "\n",
        "print('\\nPreparing val data...')\n",
        "### YOUR CODE HERE ###\n",
        "bow_val_X=vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "bow_val_y=[label for url, html, label in val_data]\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing train data...\n",
            "\n",
            "Preparing val data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWg6ZLbAJNFB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "40fbe3da-030e-4b5a-b691-e7337849ecc1"
      },
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "model.fit(bow_train_X, bow_train_y)\n",
        "\n",
        "### END CODE HERE ###\n",
        "train_y_pred = model.predict(bow_train_X)\n",
        "print('Train accuracy', accuracy_score(bow_train_y, train_y_pred))\n",
        "val_y_pred = model.predict(bow_val_X)\n",
        "print('Val accuracy', accuracy_score(bow_val_y, val_y_pred))\n",
        "\n",
        "print('')\n",
        "\n",
        "prf = precision_recall_fscore_support(bow_val_y, val_y_pred)\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.8746253746253746\n",
            "Val accuracy 0.6634304207119741\n",
            "\n",
            "Precision: 0.5844748858447488\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.7111111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIww33eRkucs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a7d4ff4-9413-4955-ff3b-2505e30e66e6"
      },
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "# Returns word vector for word if it exists, else return None.\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                          \n",
            "100%|█████████▉| 399196/400000 [00:42<00:00, 9559.01it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbM1bDocJwTy"
      },
      "source": [
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split(): \n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                ### YOUR CODE HERE ###\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec                \n",
        "                ### END CODE HERE ###\n",
        "        # We divide the sum by the number of words added, so we have the\n",
        "        # average word vector.\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "            \n",
        "    return X\n",
        "  \n",
        "glove_train_X = glove_transform_data_descriptions(train_descriptions)\n",
        "glove_train_y = [label for (url, html, label) in train_data]\n",
        "\n",
        "glove_val_X = glove_transform_data_descriptions(val_descriptions)\n",
        "glove_val_y = [label for (url, html, label) in val_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVifTxrMJ8Zb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "4b115f21-fee7-4933-8375-a004638a4b55"
      },
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "model.fit(glove_train_X, glove_train_y)\n",
        "\n",
        "### END CODE HERE ###\n",
        "train_y_pred = model.predict(glove_train_X)\n",
        "print('Train accuracy', accuracy_score(glove_train_y, train_y_pred))\n",
        "val_y_pred = model.predict(glove_val_X)\n",
        "print('Val accuracy', accuracy_score(glove_val_y, val_y_pred))\n",
        "\n",
        "print('')\n",
        "\n",
        "prf = precision_recall_fscore_support(glove_val_y, val_y_pred)\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.8656343656343657\n",
            "Val accuracy 0.7702265372168284\n",
            "\n",
            "Precision: 0.7011494252873564\n",
            "Recall: 0.8652482269503546\n",
            "F-Score: 0.7746031746031746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIDKiMPEKIO8"
      },
      "source": [
        "def train_model(train_X, train_y, val_X, val_y):\n",
        "  model = LogisticRegression(solver='liblinear')\n",
        "  model.fit(train_X, train_y)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(train_X, train_y, val_X, val_y):\n",
        "  model = train_model(train_X, train_y, val_X, val_y)\n",
        "  \n",
        "  ### YOUR CODE HERE ###\n",
        "  ### YOUR CODE HERE ###\n",
        "  train_y_pred = model.predict(train_X)\n",
        "  print('Train accuracy', accuracy_score(train_y, train_y_pred))\n",
        "  val_y_pred = model.predict(val_X)\n",
        "  print('Val accuracy', accuracy_score(val_y, val_y_pred))\n",
        "\n",
        "  print('')\n",
        "\n",
        "  prf = precision_recall_fscore_support(val_y, val_y_pred)\n",
        "  print('Precision:', prf[0][1])\n",
        "  print('Recall:', prf[1][1])\n",
        "  print('F-Score:', prf[2][1])\n",
        "  \n",
        "  ### END CODE HERE ###\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E-lU22hKcHG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "aeecce67-a27e-468a-e763-83580d0bfc2c"
      },
      "source": [
        "def prepare_data(data, featurizer):\n",
        "    X = []\n",
        "    y = []\n",
        "    for datapoint in data:\n",
        "        url, html, label = datapoint\n",
        "        # We convert all text in HTML to lowercase, so <p>Hello.</p> is mapped to\n",
        "        # <p>hello</p>. This will help us later when we extract features from \n",
        "        # the HTML, as we will be able to rely on the HTML being lowercase.\n",
        "        html = html.lower() \n",
        "        y.append(label)\n",
        "\n",
        "        features = featurizer(url, html)\n",
        "\n",
        "        # Gets the keys of the dictionary as descriptions, gets the values\n",
        "        # as the numerical features. Don't worry about exactly what zip does!\n",
        "        feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "        X.append(feature_values)\n",
        "\n",
        "    return X, y, feature_descriptions\n",
        "  \n",
        "# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword\n",
        "# to lowercase).\n",
        "def get_normalized_count(html, phrase):\n",
        "    return math.log(1 + html.count(phrase.lower()))\n",
        "\n",
        "# Returns a dictionary mapping from plaintext feature descriptions to numerical\n",
        "# features for a (url, html) pair.\n",
        "def keyword_featurizer(url, html):\n",
        "    features = {}\n",
        "    \n",
        "    # Same as before.\n",
        "    features['.com domain'] = url.endswith('.com')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.net domain'] = url.endswith('.net')\n",
        "    features['.info domain'] = url.endswith('.info')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.biz domain'] = url.endswith('.biz')\n",
        "    features['.ru domain'] = url.endswith('.ru')\n",
        "    features['.co.uk domain'] = url.endswith('.co.uk')\n",
        "    features['.co domain'] = url.endswith('.co')\n",
        "    features['.tv domain'] = url.endswith('.tv')\n",
        "    features['.news domain'] = url.endswith('.news')\n",
        "       \n",
        "    \n",
        "    keywords = ['federal','<ins','potato','trump','ww1','iframe', '<video','prayer', '<source','googlesyndication','client','<audio' ,'biden', 'clinton','sports', 'finance','awesome','high','corruption','fake news','opinion','memes','instagram','riots','save','shortcut','Rahul Gandhi','modi','lower','gotta','gimme','fact','god','holy','game','clinton','jesus','podesta','infowar','bummer','<i>','AdsbyGoogle','Advertisement','senate','whatsapp','feminism','pope','facebook','legalization','wall','weed','dogs','dog','nuclear','war','president','stupid','facebook','sold','drugs','disease','dumb','retard','asshole','comments','comment','help','Tracker','superior','link','fb','finest','nazi','jew','obama','christians','muslim','muslims','claims']\n",
        "    \n",
        "    for keyword in keywords:\n",
        "      features[keyword + ' keyword'] = get_normalized_count(html, keyword)\n",
        "\n",
        "    \n",
        "    return features\n",
        "\n",
        "keyword_train_X, train_y, _ = prepare_data(train_data, keyword_featurizer)\n",
        "keyword_val_X, val_y, _ = prepare_data(val_data, keyword_featurizer)\n",
        "train_and_evaluate_model(keyword_train_X, train_y, keyword_val_X, val_y)\n",
        "\n",
        "\n",
        "'''\n",
        "combined_train_X = combine_features([keyword_train_X, bow_train_X])\n",
        "combined_val_X = combine_features([keyword_val_X, bow_val_X])\n",
        "train_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "'''\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399196/400000 [01:00<00:00, 9559.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.9365634365634365\n",
            "Val accuracy 0.8705501618122977\n",
            "\n",
            "Precision: 0.8258064516129032\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.8648648648648649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncombined_train_X = combine_features([keyword_train_X, bow_train_X])\\ncombined_val_X = combine_features([keyword_val_X, bow_val_X])\\ntrain_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2G_QtzvKqrc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "77914f38-d271-43af-96ba-42dca0524d09"
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=300)\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(data_descriptions, vectorizer):\n",
        "  X = vectorizer.transform(data_descriptions).todense()\n",
        "  return X\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "\n",
        "train_and_evaluate_model(bow_train_X, train_y, bow_val_X, val_y)\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.8746253746253746\n",
            "Val accuracy 0.6634304207119741\n",
            "\n",
            "Precision: 0.5844748858447488\n",
            "Recall: 0.9078014184397163\n",
            "F-Score: 0.7111111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSXDmQGlLKlL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f4aa4920-6da8-4f91-ad8b-1a73278b9906"
      },
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "# Returns word vector for word if it exists, else return None.\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None\n",
        "\n",
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split(): \n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                ### YOUR CODE HERE ###\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec\n",
        "                ### END CODE HERE ###\n",
        "        # We divide the sum by the number of words added, so we have the\n",
        "        # average word vector.\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "            \n",
        "    return X\n",
        "  \n",
        "  \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "glove_train_X = glove_transform_data_descriptions(train_descriptions)\n",
        "glove_val_X = glove_transform_data_descriptions(val_descriptions)\n",
        "\n",
        "train_and_evaluate_model(glove_train_X, train_y, glove_val_X, val_y)\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.8656343656343657\n",
            "Val accuracy 0.7702265372168284\n",
            "\n",
            "Precision: 0.7011494252873564\n",
            "Recall: 0.8652482269503546\n",
            "F-Score: 0.7746031746031746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD1gl146LW2E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "2f5fdce2-f8bd-42bd-8ff2-e8178c1b0637"
      },
      "source": [
        "def combine_features(X_list):\n",
        "  return np.concatenate(X_list, axis=1)\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "combined_train_X = combine_features([keyword_train_X, bow_train_X, glove_train_X])\n",
        "combined_val_X = combine_features([keyword_val_X, bow_val_X, glove_val_X])\n",
        "\n",
        "model = train_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.9560439560439561\n",
            "Val accuracy 0.889967637540453\n",
            "\n",
            "Precision: 0.8451612903225807\n",
            "Recall: 0.9290780141843972\n",
            "F-Score: 0.8851351351351351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWFviCZ8DauJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "9a5e5758-a586-4271-eabc-629187b43bbb"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(descriptions, vectorizer):\n",
        "  X = vectorizer.transform(descriptions).todense()\n",
        "  return X\n",
        "\n",
        "print('\\nPreparing train data...')\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_train_y = [label for url, html, label in train_data]\n",
        "\n",
        "print('\\nPreparing val data...')\n",
        "### YOUR CODE HERE ###\n",
        "bow_val_X=vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "bow_val_y=[label for url, html, label in val_data]\n",
        "### END CODE HERE ###\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(data_descriptions, vectorizer):\n",
        "  X = vectorizer.transform(data_descriptions).todense()\n",
        "  return X\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "\n",
        "train_and_evaluate_model(bow_train_X, train_y, bow_val_X, val_y)\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing train data...\n",
            "\n",
            "Preparing val data...\n",
            "Train accuracy 0.8761238761238761\n",
            "Val accuracy 0.7087378640776699\n",
            "\n",
            "Precision: 0.6231884057971014\n",
            "Recall: 0.9148936170212766\n",
            "F-Score: 0.7413793103448276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOmNnnGvLpZd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "50d981b6-64ec-47f1-dc9f-8c952929f756"
      },
      "source": [
        "def prepare_data(data, featurizer):\n",
        "    X = []\n",
        "    y = []\n",
        "    for datapoint in data:\n",
        "        url, html, label = datapoint\n",
        "        # We convert all text in HTML to lowercase, so <p>Hello.</p> is mapped to\n",
        "        # <p>hello</p>. This will help us later when we extract features from \n",
        "        # the HTML, as we will be able to rely on the HTML being lowercase.\n",
        "        html = html.lower() \n",
        "        y.append(label)\n",
        "\n",
        "        features = featurizer(url, html)\n",
        "\n",
        "        # Gets the keys of the dictionary as descriptions, gets the values\n",
        "        # as the numerical features. Don't worry about exactly what zip does!\n",
        "        feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "        X.append(feature_values)\n",
        "\n",
        "    return X, y, feature_descriptions\n",
        "  \n",
        "# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword\n",
        "# to lowercase).\n",
        "def get_normalized_count(html, phrase):\n",
        "    return math.log(1 + html.count(phrase.lower()))\n",
        "\n",
        "# Returns a dictionary mapping from plaintext feature descriptions to numerical\n",
        "# features for a (url, html) pair.\n",
        "def keyword_featurizer(url, html):\n",
        "    features = {}\n",
        "    \n",
        "    # Same as before.\n",
        "    features['.com domain'] = url.endswith('.com')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.net domain'] = url.endswith('.net')\n",
        "    features['.info domain'] = url.endswith('.info')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.biz domain'] = url.endswith('.biz')\n",
        "    features['.ru domain'] = url.endswith('.ru')\n",
        "    features['.co.uk domain'] = url.endswith('.co.uk')\n",
        "    features['.co domain'] = url.endswith('.co')\n",
        "    features['.tv domain'] = url.endswith('.tv')\n",
        "    features['.news domain'] = url.endswith('.news')\n",
        "       \n",
        "    \n",
        "    keywords = ['federal','wtf','marijuania','please','<ins','potato','trump','ww1','iframe', '<video','prayer', '<source','googlesyndication','client','<audio' ,'biden', 'clinton','sports', 'finance','awesome','high','corruption','fake news','opinion','memes','instagram','riots','save','shortcut','Rahul Gandhi','modi','lower','gotta','gimme','fact','god','holy','game','clinton','jesus','podesta','infowar','bummer','<i>','AdsbyGoogle','Advertisement','senate','whatsapp','feminism','pope','facebook','legalization','wall','weed','dogs','dog','nuclear','war','president','stupid','facebook','sold','drugs','disease','dumb','retard','asshole','comments','comment','help','Tracker','superior','link','fb','finest','nazi','jew','obama','christians','muslim','muslims','claims']\n",
        "    \n",
        "    for keyword in keywords:\n",
        "      features[keyword + ' keyword'] = get_normalized_count(html, keyword)\n",
        "\n",
        "    \n",
        "    return features\n",
        "\n",
        "keyword_train_X, train_y, _ = prepare_data(train_data, keyword_featurizer)\n",
        "keyword_val_X, val_y, _ = prepare_data(val_data, keyword_featurizer)\n",
        "'''\n",
        "train_and_evaluate_model(keyword_train_X, train_y, keyword_val_X, val_y)\n",
        "'''\n",
        "\n",
        "\n",
        "combined_train_X = combine_features([keyword_train_X, bow_train_X])\n",
        "combined_val_X = combine_features([keyword_val_X, bow_val_X])\n",
        "train_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy 0.957042957042957\n",
            "Val accuracy 0.9061488673139159\n",
            "\n",
            "Precision: 0.8733333333333333\n",
            "Recall: 0.9290780141843972\n",
            "F-Score: 0.9003436426116839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu8h9wNZcm7y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffe4ece6-7193-4fc9-a806-cf2755ac2610"
      },
      "source": [
        "#@title Live Fake News Classification Demo { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "def get_data_pair(url):\n",
        "  if not url.startswith('http'):\n",
        "      url = 'http://' + url\n",
        "  url_pretty = url\n",
        "  if url_pretty.startswith('http://'):\n",
        "      url_pretty = url_pretty[7:]\n",
        "  if url_pretty.startswith('https://'):\n",
        "      url_pretty = url_pretty[8:]\n",
        "      \n",
        "  # Scrape website for HTML\n",
        "  response = requests.get(url, timeout=10)\n",
        "  htmltext = response.text\n",
        "  \n",
        "  return url_pretty, htmltext\n",
        "\n",
        "curr_url = \"gulfnews.com\" #@param {type:\"string\"}\n",
        "\n",
        "url, html = get_data_pair(curr_url)\n",
        "\n",
        "# Call on the output of *keyword_featurizer* or something similar\n",
        "# to transform it into a format that allows for concatenation. See\n",
        "# example below.\n",
        "def dict_to_features(features_dict):\n",
        "  X = np.array(list(features_dict.values())).astype('float')\n",
        "  X = X[np.newaxis, :]\n",
        "  return X\n",
        "def featurize_data_pair(url, html):\n",
        "  # Approach 1.\n",
        "  keyword_X = dict_to_features(keyword_featurizer(url, html))\n",
        "  \n",
        "  \n",
        "  # Approach 2.\n",
        "  description = get_description_from_html(html)\n",
        "  \n",
        "  bow_X = vectorize_data_descriptions([description], vectorizer)\n",
        "  \n",
        "  # Approach 3.\n",
        "  '''\n",
        "  glove_X = glove_transform_data_descriptions([description])\n",
        "  '''\n",
        "  X = combine_features([keyword_X, bow_X])\n",
        "   \n",
        "  return X\n",
        "\n",
        "curr_X = featurize_data_pair(url, html)\n",
        "\n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "curr_y = model.predict(curr_X)[0]\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "if curr_y < 0.5 :\n",
        "  print(curr_url, 'appears to be real.')\n",
        "else:\n",
        "  print(curr_url, 'appears to be fake.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gulfnews.com appears to be real.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTqHvYzGnpYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "956876e7-0dd5-4835-c070-8880ee70fe6b"
      },
      "source": [
        "\n",
        "### PUT TEST CODE HERE ###\n",
        "\n",
        "with open(os.path.join(basepath, 'test_data.pkl'), 'rb') as f:\n",
        "  test_data = pickle.load(f)\n",
        "  \n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "print('Loading test data...')\n",
        "test_X = []\n",
        "for url, html, label in test_data:\n",
        "  curr_X = np.array(featurize_data_pair(url, html))\n",
        "  test_X.append(curr_X[0])\n",
        "  \n",
        "test_X = np.array(test_X)\n",
        "\n",
        "test_y = [label for url, html, label in test_data]\n",
        "\n",
        "print('Done loading test data...')\n",
        "\n",
        "test_y_pred = model.predict(test_X)\n",
        "\n",
        "print('Test accuracy', accuracy_score(test_y, test_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(test_y, test_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(test_y, test_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])\n",
        "  \n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading test data...\n",
            "Done loading test data...\n",
            "Test accuracy 0.7886178861788617\n",
            "Confusion matrix:\n",
            "[[ 83  51]\n",
            " [  1 111]]\n",
            "Precision: 0.6851851851851852\n",
            "Recall: 0.9910714285714286\n",
            "F-Score: 0.8102189781021897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyAZrW36e90H"
      },
      "source": [
        "# Runtime code for N.A.T.E"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZHGg1BDfCJQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "da41c73b-9403-4f29-877d-c11bb32ec88f"
      },
      "source": [
        "#@title :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: N.A.T.E :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "import time\n",
        "import sys\n",
        "'''\n",
        "def my_except_hook(exctype, value, traceback):\n",
        "        print('There has been an error in the system')\n",
        "sys.excepthook = my_except_hook\n",
        "'''\n",
        "print('Hello I am N.A.T.E')\n",
        "print('I am here to help you discern fake news websites from real news websites...')\n",
        "time.sleep(2)\n",
        "print('')\n",
        "print('Copy and paste the website url here!')\n",
        "\n",
        "\n",
        "\n",
        "def get_data_pair(url):\n",
        "  if not url.startswith('http'):\n",
        "      url = 'http://' + url\n",
        "  url_pretty = url\n",
        "  if url_pretty.startswith('http://'):\n",
        "      url_pretty = url_pretty[7:]\n",
        "  if url_pretty.startswith('https://'):\n",
        "      url_pretty = url_pretty[8:]\n",
        "  # Scrape website for HTML\n",
        "  response = requests.get(url, timeout=10)\n",
        "  htmltext = response.text\n",
        "  \n",
        "  return url_pretty, htmltext\n",
        "\n",
        "curr_url = input(str(' ')) \n",
        "print('')\n",
        "url, html = get_data_pair(curr_url)\n",
        "\n",
        "# Call on the output of *keyword_featurizer* or something similar\n",
        "# to transform it into a format that allows for concatenation. See\n",
        "# example below.\n",
        "def dict_to_features(features_dict):\n",
        "  X = np.array(list(features_dict.values())).astype('float')\n",
        "  X = X[np.newaxis, :]\n",
        "  return X\n",
        "def featurize_data_pair(url, html):\n",
        "  # Approach 1.\n",
        "  keyword_X = dict_to_features(keyword_featurizer(url, html))\n",
        "  \n",
        "  \n",
        "  # Approach 2.\n",
        "  description = get_description_from_html(html)\n",
        "  \n",
        "  bow_X = vectorize_data_descriptions([description], vectorizer)\n",
        "  \n",
        "  # Approach 3.\n",
        "  '''\n",
        "  glove_X = glove_transform_data_descriptions([description])\n",
        "  '''\n",
        "  X = combine_features([keyword_X, bow_X])\n",
        "   \n",
        "  return X\n",
        "\n",
        "curr_X = featurize_data_pair(url, html)\n",
        "\n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "curr_y = model.predict(curr_X)[0]\n",
        "\n",
        "print('processing...')\n",
        "time.sleep(0.5)\n",
        "print('')\n",
        "print('As far as I can detect the content in the url: ')\n",
        "\n",
        "if curr_y < 0.5 :\n",
        "  print(curr_url, 'appears to be real.')\n",
        "else:\n",
        "  print(curr_url, 'appears to be fake.')\n",
        "\n",
        "time.sleep(3)\n",
        "print('')\n",
        "print('                           Always stay vigilant for fake news...')\n",
        "time.sleep(1)\n",
        "print('                                 And for now... Goodbye!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello I am N.A.T.E\n",
            "I am here to help you discern fake news websites from real news websites...\n",
            "\n",
            "Copy and paste the website url here!\n",
            " https://gulfnews.com/entertainment/bollywood/bollywood-actor-irrfan-khan-dies-at-53-1.71231469\n",
            "\n",
            "processing...\n",
            "\n",
            "As far as I can detect the content in the url: \n",
            "https://gulfnews.com/entertainment/bollywood/bollywood-actor-irrfan-khan-dies-at-53-1.71231469 appears to be real.\n",
            "\n",
            "                           Always stay vigilant for fake news...\n",
            "                                 And for now... Goodbye!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}